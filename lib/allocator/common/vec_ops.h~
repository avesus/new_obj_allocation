#ifndef _VEC_OPS_H_
#define _VEC_OPS_H_

uint64_t
create_full_mask(const uint32_t * const ptrs, uint32_t idx_mask) {
    uint64_t mask = 0;
    do {
        uint32_t pidx = bits::tzcnt<uint32_t>(idx_mask);
        idx_mask ^= (1 << pidx);

        mask |= (1UL) << ptrs[pidx];
    } while (idx_mask);
    return idx_mask;
}

uint64_t
create_partial_mask(const uint32_t * const ptrs, uint32_t idx_mask) {
    return create_full_mask(ptrs, idx_mask & 0xff);
}


__m256i
merge_partial_masks_v(const __m256i m1, const __m256i m2) {
    return __mm256i_or_si256(m1, m2);
}


uint64_t
reduce_mask_v(const __m256i m) {
    _m128i m128 =
        _mm_or_si128(_mm256_castsi256_si128(m), _mm256_extracti128_si256(m), 1);
    __m128i m128_reduced = _mm_or_si128(m128, _mm_unpackhi_epi64(m128, m128));
    return (uint64_t)_mm_cvtsi128_si64(m128_reduced);
}

template<uint32_t slab_size>
get_indexes(__m256i ptrs_v) {
    return vmath::vdiv<uint32_t, slab_size>(ptrs_v);
}

template<uint32_t slab_size>
update_ptrs(__m256i ptrs_v, __m256i idx_v) {
    __m256i bases_v = vmath::vmult<uint32_t, slab_size>(idx_v);
    return _mm256_sub_epu32(ptrs_v, bases_v);
}



__m256i
create_packed_idx_vec_v(__m256i idx_v1, __m256i idx_v2) {
    return _mm256_packus_epi16(idx_v1, idx_v2);
}

__m256i
create_partial_mask_v(__m256i idx_v) {
    __m256i indexes_lo = _mm256_and_epi64(idx_v, _m256_set1_epi64x(0xffffffff));
    __m256i indexes_hi = _mm256_shuffle_epi32(idx_v, 0x5555);

    __m256i mask_lo = _mm256_sllv_epi64(_mm256_set1_epi64x(1), indexes_lo);
    __m256i bvec_hi = _mm256_sllv_epi64(_mm256_set1_epi64x(1), indexes_hi);

    return _mm256_or_epi64(mask_lo, mask_hi);
}

uint64_t
create_free_mask_v_nb(__m256i idx_v1,
                      __m256i idx_v2,
                      __m256i idx_v3,
                      __m256i idx_v4) {
    
    __m256i m1 = create_partial_mask_v(ptrs + 0, base);
    __m256i m2 = create_partial_mask_v(ptrs + 8, base);
    __m256i m3 = create_partial_mask_v(ptrs + 16, base);
    __m256i m4 = create_partial_mask_v(ptrs + 24, base);

    __m256i m12 = merge_partial_masks_v(m1, m2);
    __m256i m34 = merge_partial_masks_v(m3, m4);

    return reduce_mask_v(merge_partial_masks_v(m12, m34));
}


#endif
